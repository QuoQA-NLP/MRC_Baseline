# 2022ë…„ ì¸ê³µì§€ëŠ¥ ì˜¨ë¼ì¸ ê²½ì§„ëŒ€íšŒ / ë¬¸ì„œ ê²€ìƒ‰ íš¨ìœ¨í™”ë¥¼ ìœ„í•œ ê¸°ê³„ë…í•´ ë¬¸ì œ - íŒ€: QuoQA


## í”„ë¡œì íŠ¸ ê°œìš”


í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ ë³¸ë¬¸ì—ì„œ ì§ˆë¬¸ì˜ ë‹µì„ ì°¾ëŠ” ê³¼ì œì…ë‹ˆë‹¤. ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ì™€ ë‹µë³€ì´ ê°€ëŠ¥í•œ ê²½ìš°ê°€ ëª¨ë‘ ì¡´ì¬í•˜ë©°, Exact Matchë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ê¸°ê³„ë…í•´ëŠ” Extractive Question Answering í˜•ì‹ìœ¼ë¡œì¨ Context ì•ˆì—ì„œ Answer Spanì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.


## ì‚¬ìš©ë°©ë²•ë¡  ë° ì¬í˜„ ëª…ë ¹ì–´

- í•´ë‹¹ ë¬¸ë‹¨ìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥ì„±ì„ íŒë‹¨í•˜ëŠ” ê²ƒê³¼ ë‹µë³€ ë¬¸ìì—´ì„ ì¶”ì¶œí•˜ëŠ” ê³¼ì •ì„ Transformer backbone model ë‹¨ì¼ ëª¨ë¸ë¡œì¨ ë™ì‹œì— ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤.
- ë‹µë³€ ê°€ëŠ¥ì„±ì„ ê¸°ì¤€ìœ¼ë¡œ ì‚°ì¶œí•œ lossì™€ ë¬¸ìì—´ ì‹œì‘ì , ëì ì´ ì¼ì¹˜í•˜ëŠ”ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚°ì¶œí•œ lossë¥¼ ê°€ì¤‘í‰ê· í•©í•˜ì—¬ [total loss](./models/roberta.py)ë¥¼ ì‚°ì •í•©ë‹ˆë‹¤.
- í•œì •ëœ GPU VRAM ìì›ì—ì„œ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•˜ì—¬ Gradient Accumulation, Gradient Checkpointë¥¼ ì‚¬ìš©í–ˆìœ¼ë©° ì´ë¥¼ í†µí•´ ì„±ëŠ¥ í–¥ìƒì„ ì´ë¤„ëƒˆìŠµë‹ˆë‹¤.


**í›ˆë ¨ ëª…ë ¹ì–´**
`bash running_train_only.sh`

**ì¶”ë¡  ëª…ë ¹ì–´**
`bash running_inference_only.sh`


## ê¸°í•™ìŠµê°€ì¤‘ì¹˜(Pretrained Language Model)

KLUE: Korean Language Understanding Evaluation(2021)ì—ì„œ ê³µê°œí•œ roberta-large ëª¨ë¸ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. (arXiv:2105.09680)

RoBERTa ëª¨ë¸ì„ ì„ ì •í•œ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
1. ë‹µë³€ ë¶ˆê°€ í•­ëª©ê³¼ ì‘ë‹µ ë¬¸ìì—´ì„ ë²¤ì¹˜ë§ˆí¬ë¡œ ì‚¼ì€ [SQuAD v2.0 Benchmark](https://paperswithcode.com/sota/question-answering-on-squad20), [KLUE Benchmark](https://klue-benchmark.com/tasks/72/leaderboard/task)ì—ì„œ Roberta Backboneì´ ì„±ëŠ¥ì´ ì¢‹ë‹¤ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.
2. #Trainable Paramsì™€ Num Layersë¥¼ ë”°ì¡Œì„ ë•Œ RoBERTa-large ëª¨ë¸ì´ KPFBert-base ë“±ê³¼ ê°™ì€ base size ëª¨ë¸ì— ë¹„í•´ì„œ ë”¥ëŸ¬ë‹ í•™ìŠµì— ë¹„êµìš°ìœ„ê°€ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒí–ˆìŠµë‹ˆë‹¤.
3. íŒ€ ìì²´ì ìœ¼ë¡œ Train Datasetì„ 5 Foldë¡œ ë‚˜ëˆ ì„œ Evaluation Scoreì„ ì‚°ì¶œí–ˆì„ ë•Œ klue/roberta-largeê°€ ì„±ëŠ¥ì´ ì œì¼ ìš°ìˆ˜í•˜ê²Œ ë‚˜ì™”ìŠµë‹ˆë‹¤.

êµ¬ì²´ì ìœ¼ë¡œ Huggingfaceì— ì—…ë¡œë“œëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤: [ğŸ”— klue/roberta-large](https://huggingface.co/klue/roberta-large)

í•´ë‹¹ pre-trained weightëŠ” 2021ë…„ 06ì›” 15ì¼ì— ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤. í•´ë‹¹ PLMì€ ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ì…‹, í† í¬ë‚˜ì´ì €, ëª¨ë¸ êµ¬ì¡°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í›ˆë ¨ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.

- Pretrained Corpora (ì´ 62GB)
    - MODU Corpus
        - Korean Corpus containing formal articles and colloquial text released by the National Institute of Korean Language
    - CC-100-Kor
        - Korean portion of the multilingual web crawled corpora used for training XLM-R
    - NAMUWIKI
        - Korean web-based encyclopedia
    - NEWSCRAWL
        - Collection of 12,800,000 news articles from 2011 to 2020
    - PETITION
        - Blue House National Petition: collection of public petitions
- Tokenizer
    - 32K Vocab Size
    - Morpheme-based subword tokenization
    - Pre-tokenize raw-text into morphemes and then apply BPE
- Model Structure
    - 24 transformer layers
    - 337M trainable parameters
    - Dynamic / WWM Masking



## ë°ì´í„°ì…‹

```
../DATA
|    +- sample_submission.csv
|    +- test.json
|    +- train.json
```

    - 'train.json'ë¥¼ Huggingfaceì˜ datasets.Dataset í´ë˜ìŠ¤ë¡œ ë³€í™˜í•œë‹¤.
    - Dataset í´ë˜ìŠ¤ë¡œ ë³€í™˜ëœ train datasetì„ ë°”íƒ•ìœ¼ë¡œ RobertaForV2QuestionAnsweringì„ íŒŒì¸íŠœë‹ì„ ì§„í–‰í•œë‹¤.
    - 'test.json'ë¥¼ Huggingfaceì˜ datasets.Dataset í´ë˜ìŠ¤ë¡œ ë³€í™˜í•œë‹¤.
    - ì•ì„œ Finetuningí•œ RobertaForV2QuestionAnswering ëª¨ë¸ì„ ë°”íƒ•ìœ¼ë¡œ 'FINAL_SUBMISSION.csv' íŒŒì¼ì„ ìƒì„±í•œë‹¤.
    
## í•˜ë“œì›¨ì–´

`CPU 10C, Nvidia T4 GPU x 1, 90MEM, 1TB`

## ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
USER/
â”œâ”€â”€ running_train_only.sh
â”œâ”€â”€ running_inference_only.sh
â”œâ”€â”€ train.py
â”œâ”€â”€ inference.py
â”œâ”€â”€ trainer.py
â”œâ”€â”€ arguments.py
â”œâ”€â”€ question_ids.json
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ models
â”‚Â Â  â”œâ”€â”€ roberta.py
â”‚Â Â  â”œâ”€â”€ output.py
â”‚Â Â  â”œâ”€â”€ bart.py
â”‚Â Â  â”œâ”€â”€ bert.py
â”‚Â Â  â””â”€â”€ electra.py
â”‚
â”œâ”€â”€ utils
â”‚Â Â  â”œâ”€â”€ encoder.py
â”‚Â Â  â”œâ”€â”€ loader.py
â”‚Â Â  â”œâ”€â”€ preprocessor.py
â”‚Â Â  â”œâ”€â”€ postprocessor.py
â”‚Â Â  â””â”€â”€ metric.py
â”‚
â”œâ”€â”€ exps
â”‚Â Â  â”œâ”€â”€ checkpoint-125/ *í•˜ë‹¨ ìƒì„¸ ê¸°ìˆ *
â”‚Â Â  â”œâ”€â”€ checkpoint-250/
â”‚Â Â  â”œâ”€â”€ checkpoint-375/
â”‚Â Â  â”œâ”€â”€ checkpoint-500/
â”‚Â Â  â”œâ”€â”€ checkpoint-625/
â”‚Â Â  â”œâ”€â”€ checkpoint-750/
â”‚Â Â  â””â”€â”€ checkpoint-875/
â”‚
â””â”€â”€ RESULT * Output ìƒì„¸ì„¤ëª… *
 Â Â  â”œâ”€â”€ final_submission.csv
    â””â”€â”€ checkpoint-875
        â”œâ”€â”€ pytorch_model.bin
        â”œâ”€â”€ config.json
        â”œâ”€â”€ optimizer.pt
        â”œâ”€â”€ rng_state.pth
        â”œâ”€â”€ scheduler.pt
        â”œâ”€â”€ special_tokens_map.json
        â”œâ”€â”€ tokenizer_config.json
        â”œâ”€â”€ tokenizer.json
        â”œâ”€â”€ trainer_state.json
        â”œâ”€â”€ training_args.bin
        â””â”€â”€ vocab.txt
```


- `running_train_only.sh`

  - ëª¨ë¸ í•™ìŠµí•˜ê¸° ìœ„í•œ shell script íŒŒì¼ì…ë‹ˆë‹¤.
  - í›ˆë ¨ì— í•„ìš”í•œ argumentëŠ” ì•„ë˜ë¥¼ ì°¸ì¡°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

- `running_inference_only.sh`

  - ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ë¡œ ì¶”ë¡ í•˜ê¸° ìœ„í•œ shell script íŒŒì¼ì…ë‹ˆë‹¤.
  - ì¶”ë¡ ì— í•„ìš”í•œ argumentëŠ” ì•„ë˜ë¥¼ ì°¸ì¡°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

- `train.py`

  - ëª¨ë¸ í•™ìŠµì„ ì‹¤í–‰í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.
  - ì €ì¥ëœ model checkpoint ê°€ì¤‘ì¹˜ íŒŒì¼ì€ `exps/` í´ë”ì— ìˆìŠµë‹ˆë‹¤.
  - ìµœì¢… ì¶”ë¡ ì— ì“°ì´ëŠ” ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì€ `RESULT/` í´ë”ì— ìˆìŠµë‹ˆë‹¤.

- `inference.py`

  - í•™ìŠµëœ model ê°€ì¤‘ì¹˜ë¥¼ í†µí•´ predictioní•˜ê³ , ì˜ˆì¸¡í•œ ê²°ê³¼ë¥¼ csv íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.
  - ì €ì¥ëœ ìµœì¢… submission íŒŒì¼ì€ `RESULT/` í´ë”ì— ìˆìŠµë‹ˆë‹¤.

- `trainer.py`

  - Huggingfaceì˜ Trainer classë¥¼ ìƒì†ë°›ì•„ trainerë¥¼ êµ¬í˜„í•œ íŒŒì¼ì…ë‹ˆë‹¤.
  - compute_loss, evaluate, predict í•¨ìˆ˜ë¥¼ customí•˜ê²Œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤. 

- `arguments.py`

  - í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ arguments ê´€ë ¨ classë¥¼ ì •ì˜í•œ íŒŒì¼ì…ë‹ˆë‹¤.
  - argumentsì˜ ì¢…ë¥˜, ê¸°ë³¸ê°’, help message ë“±ì„ ì •ì˜í–ˆìŠµë‹ˆë‹¤.

- `question_ids.json`

  - ì¬í˜„ì„ ìœ„í•´ì„œ í•™ìŠµí•  ë•Œì˜ train dataì˜ id ë¦¬ìŠ¤íŠ¸ë¥¼ ì €ì¥í•˜ê³  ì´ë¥¼ ì´ìš©í•´ì„œ /DATA/train.jsonì— ìˆëŠ” ë°ì´í„°ë¥¼ ì •ë ¬í•˜ì˜€ìŠµë‹ˆë‹¤.

- `models/`

  - ëª¨ë¸ classë¥¼ êµ¬í˜„í•œ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ì…ë‹ˆë‹¤.
	- ìµœì¢… ëª¨ë¸ì€ `roberta.py`ì— ìˆëŠ” RobertaForV2QuestionAnswering classë§Œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
	- ì´ ì™¸ì— `output.py`ì—ì„œ ëª¨ë¸ ì¶œë ¥ë¬¼ classë¥¼ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

- `utils/`
	  - ë°ì´í„°ì…‹ ì „ì²˜ë¦¬, ëª¨ë¸ ì…ë ¥ ë°ì´í„° ì „/í›„ì²˜ë¦¬, í‰ê°€ì§€í‘œ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ì…ë‹ˆë‹¤.
	- `encoder.py`
		- ë°ì´í„°ë¥¼ tokenizeí•˜ê³  is_impossible, ì •ë‹µ index ë“±ì„ êµ¬í•˜ëŠ” Encoder classë¥¼ ì •ì˜í•œ íŒŒì¼ì…ë‹ˆë‹¤.
	- `loader.py`
	    - train, test ë°ì´í„°ê°€ ìˆëŠ” /DATA ë””ë ‰í† ë¦¬ì—ì„œ json íŒŒì¼ì¸ ì›ì‹œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  Huggingfaceì˜ Datasets í´ë˜ìŠ¤ì— ë§ê²Œ í˜•ì‹ì„ ë³€í˜•í•˜ëŠ” í´ë˜ìŠ¤ê°€ ìˆëŠ” íŒŒì¼ì…ë‹ˆë‹¤.
	- `preprocessor.py`
		- ì •ë‹µì´ ì—†ëŠ” ê²½ìš°, ì •ë‹µì´ 2ê°œ ì´ìƒì¸ ê²½ìš°ë¥¼ ì²˜ë¦¬í•˜ëŠ” Preprocessor classë¥¼ ì •ì˜í•œ íŒŒì¼ì…ë‹ˆë‹¤.
	- `postprocessor.py`
		- ëª¨ë¸ ì¶œë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… predictionì„ êµ¬í•˜ê³  í¬ë§·ì— ë§ì¶° ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•œ íŒŒì¼ì…ë‹ˆë‹¤.
		- Konlpyì˜ í˜•íƒœì†Œ ë¶„ì„ê¸° mecabì„ í™œìš©í•˜ì—¬ í˜•íƒœì†Œ ë¶„ì„ í›„, ëì— ì¡°ì‚¬ ë° ì•ë’¤ì— íŠ¹ìˆ˜ ë¬¸ì ì œê±° (mecab version: mecab of 0.996/ko-0.9.2)
	- `metric.py`
		- ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•œ í‰ê°€ì§€í‘œ Metric classë¥¼ ì •ì˜í•œ íŒŒì¼ì…ë‹ˆë‹¤.


- `exps/`

    - train.pyë¥¼ ì‹¤í–‰í•  ì‹œ, í›ˆë ¨ë  ë•Œë§ˆë‹¤ ìƒì„±ë˜ëŠ” ëª¨ë¸ checkpointë¥¼ ì €ì¥í•˜ëŠ” ë””ë ‰í† ë¦¬ì…ë‹ˆë‹¤.

- `RESULT/`
    
    - train.pyë¥¼ í†µí•´ í•™ìŠµëœ ìµœì¢… ëª¨ë¸ checkpoint ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì €ì¥í•˜ëŠ” ë””ë ‰í† ë¦¬ì…ë‹ˆë‹¤.

    - inference.pyë¥¼ í†µí•´ Test dataì— ëŒ€í•´ì„œ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë””ë ‰í† ë¦¬ì…ë‹ˆë‹¤.

    - `final_submission.csv`
        - ìµœì¢… ì˜ˆì¸¡ê°’ì´ ì €ì¥ëœ submission íŒŒì¼ì…ë‹ˆë‹¤.
        
    - `checkpoint-875/`
        - ìµœì¢… ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ì…ë‹ˆë‹¤.
        - `pytorch_model.bin`
            - ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ì €ì¥ëœ íŒŒì¼ì…ë‹ˆë‹¤.
        - `config.json`
            - ëª¨ë¸ì— ëŒ€í•œ ì „ë°˜ì ì¸ íŠ¹ì§• ë° ê²½ë¡œê°€ ì í˜€ìˆëŠ” íŒŒì¼ì…ë‹ˆë‹¤.
        - `optimizer.pt`
            - optimizer weightë¥¼ ë‹´ì€ íŒŒì¼ì…ë‹ˆë‹¤.
        - `rng_state.pth`
            - python, numpy, cpu ì •ë³´ë¥¼ ë‹´ì€ íŒŒì¼ì…ë‹ˆë‹¤.
        - `scheduler.pt`
            - scheduler weightë¥¼ ë‹´ì€ íŒŒì¼ì…ë‹ˆë‹¤.
        - `special_tokens_map.json`
            - tokenizerì—ì„œ ì‚¬ìš©í•˜ëŠ” special tokenì„ ë‹´ì€ íŒŒì¼ì…ë‹ˆë‹¤.
        - `tokenizer_config.json`
            - tokenizerì˜ special token, class ë° ëª¨ë¸ ì´ë¦„ ì •ë³´ ë‹´ì€ íŒŒì¼ì…ë‹ˆë‹¤.
        - `tokenizer.json`
            - tokenizerì˜ ê° vocab id ì •ë³´ë¥¼ ë‹´ì€ íŒŒì¼ì…ë‹ˆë‹¤.
        - `trainer_state.json`
            - ê° log step ë‹¹, learning rateë‚˜ loss, eval ì •ë³´ ë“±ì„ ë‹´ì€ íŒŒì¼ì…ë‹ˆë‹¤.
        - `training_args.bin`
            - train argumentë¥¼ ë‹´ì€ íŒŒì¼ì…ë‹ˆë‹¤.
        - `vocab.txt`
            - tokenizerì— ë‹¤ë£¨ëŠ” ë¬¸ìë“¤ì„ ë‹´ì€ íŒŒì¼ì…ë‹ˆë‹¤.


## Arguments

### running_train_only.sh Argument ì„¤ëª…

|      argument       | description                                                                                   |
| :-----------------: | :-------------------------------------------------------------------------------------------- | 
| do_train|ëª¨ë¸ì„ í›ˆë ¨í• ì§€ ì—¬ë¶€ ê²°ì •í•©ë‹ˆë‹¤.|
| group_name|wandb ê·¸ë£¹ ì´ë¦„ ì§€ì •í•©ë‹ˆë‹¤.|
|data_path|Nipa dataset ì„ íƒí•©ë‹ˆë‹¤.|
|use_validation|validationì„ ìˆ˜í–‰í• ì§€ ì—¬ë¶€ ê²°ì •|
|PLM|ëª¨ë¸ PLM ê²°ì •í•©ë‹ˆë‹¤.|
|model_category|models í´ë” ì•ˆì— ì‚¬ìš©í•  íŒŒì¼ ì„ íƒí•©ë‹ˆë‹¤.|
|model_name|model_categoryì—ì„œ ì„ íƒí•œ íŒŒì¼ì—ì„œ ì„¸ë¶€ class ì„ íƒí•©ë‹ˆë‹¤.|
|max_length|ìµœëŒ€ ê¸¸ì´ ì§€ì •í•©ë‹ˆë‹¤.|
|save_strategy|step or epoch ê¸°ì¤€ ë“±ìœ¼ë¡œ ì €ì¥í•˜ëŠ” ë°©ì‹ì„ ì •í•©ë‹ˆë‹¤.|
|save_total_limit|ìµœëŒ€ checkpoint ì €ì¥ ê°¯ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.|
|learning_rate|í›ˆë ¨ learning rateë¥¼ ì§€ì •í•©ë‹ˆë‹¤.|
|per_device_train_batch_size|train batch sizeë¥¼ ì§€ì •í•©ë‹ˆë‹¤.|
|per_device_eval_batch_size|eval batch sizeë¥¼ ì§€ì •í•©ë‹ˆë‹¤.|
|gradient_accumulation_steps|gradient accumulation ìˆ˜ë¥¼ ì •í•©ë‹ˆë‹¤.|
|gradient_checkpointing|gradient checkpoint ì—¬ë¶€ë¥¼ ì •í•©ë‹ˆë‹¤.|
|max_steps|í•™ìŠµ ìµœëŒ€ stepì„ ì§€ì •í•©ë‹ˆë‹¤.|


### running_inference_only.sh Argument ì„¤ëª…

|      argument       | description                                                                                   |
| :-----------------: | :-------------------------------------------------------------------------------------------- | 
| do_predict|ì£¼ì–´ì§„ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡í• ì§€ ë§ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.|
|PLM|ì›í•˜ëŠ” ê°€ì¤‘ì¹˜ ëª¨ë¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.|
|model_category|models í´ë” ì•ˆì— ì‚¬ìš©í•  íŒŒì¼ ì„ íƒí•©ë‹ˆë‹¤.|
|model_name|model_categoryì—ì„œ ì„ íƒí•œ íŒŒì¼ì—ì„œ ì„¸ë¶€ class ì„ íƒí•©ë‹ˆë‹¤.|
|max_length|ìµœëŒ€ ê¸¸ì´ ì§€ì •í•©ë‹ˆë‹¤.|
|output_dir|ì˜ˆì¸¡ê°’ì„ ì €ì¥í•  ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.|
|file_name|ì˜ˆì¸¡ê°’ì— ëŒ€í•œ íŒŒì¼ ì´ë¦„ì„ ì§€ì •í•©ë‹ˆë‹¤.|